{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fed-Averaging Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHio9xLUgj3+xbbEH21i8j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thehimalayanleo/Private-Machine-Learning/blob/master/Fed_Averaging_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_Ysx3k6cncR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import copy\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA09uCudcqI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
        "        self.fc = nn.Linear(1024, 10)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = x.view(-1, 784)\n",
        "        #print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        #print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        #print(x.shape)\n",
        "        x = x.view(-1, 1024)\n",
        "        #print(x.shape)\n",
        "        x = F.relu(self.fc(x))\n",
        "        #print(x.shape)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "060pUpykwkBD",
        "colab_type": "code",
        "outputId": "d842bc21-a133-4ef7-f0fc-4ac3053c366c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "num_epochs = 10\n",
        "lr = 0.01\n",
        "batch_size = 64\n",
        "batch_size_test = 1000\n",
        "log_interval = 10\n",
        "use_gpu = 0\n",
        "device = torch.device('cuda:{}'.format(use_gpu) if torch.cuda.is_available() and use_gpu != -1 else 'cpu')\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f242140f410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CpIsEJaw2L1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('../data/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZOWnEGVgrlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Net()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuSVVCNPhEVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  net.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = net(data)\n",
        "      #print(target.shape)\n",
        "      #print(output.shape)\n",
        "      test_loss+=F.nll_loss(output, target).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      #print(pred.shape)\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "  test_loss/=len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('Test Set:  Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(test_loss, correct, len(test_loader.dataset),\n",
        "                                                                                            100.*correct/len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aNx_yh9xHAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Splitting dataset using indices\n",
        "\n",
        "class DatasetSplit(Dataset): \n",
        "  def __init__(self, dataset, indices):\n",
        "    self.dataset = dataset\n",
        "    self.indices = list(indices)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.indices)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    image, label = self.dataset[self.indices[item]]\n",
        "    return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPypWPeuHxcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LocalUpdate:\n",
        "  def __init__(self, dataset=None, indices=None):\n",
        "    #self.args = args\n",
        "    self.train_loader = DataLoader(DatasetSplit(dataset, indices), batch_size=batch_size, shuffle=True)\n",
        "  \n",
        "  def train(self, net):\n",
        "    #net = Net()\n",
        "    epoch_losses = []\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.5)\n",
        "    net.train()\n",
        "    for iters in range(num_epochs):\n",
        "      batch_loss = []\n",
        "      for batch_indx, (data, target) in enumerate(self.train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = net(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        if batch_indx % log_interval == 0:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(iters, batch_indx*len(data), len(self.train_loader.dataset), \n",
        "                                                                      100.*batch_indx/len(self.train_loader), loss.item()))\n",
        "        batch_loss.append(loss.item())\n",
        "      epoch_losses.append(sum(batch_loss)/len(batch_loss))\n",
        "    return net.state_dict(), sum(epoch_losses)/len(epoch_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fypu7CloMofp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iid_dataset(dataset, num_users):\n",
        "  dict_users = {}\n",
        "  all_indices = [indx for indx in range(len(dataset))]\n",
        "  num_items = int(len(dataset)/num_users)\n",
        "  for user in range(num_users):\n",
        "    dict_users[user] = set(np.random.choice(all_indices, num_items, replace=False))\n",
        "    all_indices = list(set(all_indices)-dict_users[user])\n",
        "  return dict_users"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NT6-vFAZsme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True,\n",
        "                                   transform=transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "num_users = 10\n",
        "dict_users_mnist_iid = iid_dataset(dataset_train, num_users)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq6GjBjncZX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Store global weights\n",
        "\n",
        "global_net = Net().to(device)\n",
        "global_weight = global_net.state_dict()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beQGybaTglDR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f07059a-c081-4abe-b25d-03189c1734fb"
      },
      "source": [
        "frac = 0.1\n",
        "loss_training = []\n",
        "for iter in range(num_epochs):\n",
        "  local_weights, local_loss = [], []\n",
        "  users_max = max(int(frac*num_users), 1)\n",
        "  user_indices = np.random.choice(range(num_users), users_max, replace=False)\n",
        "  for indx in user_indices:\n",
        "    local_update = LocalUpdate(dataset=dataset_train, indices=dict_users_mnist_iid[indx])\n",
        "    weight, loss = local_update.train(net=copy.deepcopy(global_net).to(device))\n",
        "    local_weights.append(copy.deepcopy(weight))\n",
        "    local_loss.append(copy.deepcopy(loss))\n",
        "    weight_average = copy.deepcopy(local_weights[0])\n",
        "    for key in weight_average.keys():\n",
        "      for indx in range(1, len(local_weights)):\n",
        "        weight_average[key] += local_weights[indx][key]\n",
        "      weight_average[key] = torch.div(weight_average[key], len(local_weights))\n",
        "    global_weight = weight_average\n",
        "    global_net.load_state_dict(global_weight)\n",
        "\n",
        "    loss_average = sum(local_loss)/len(local_loss)\n",
        "    print(\"Iteration {:3d}, Average Loss {:.2f}\".format(iter, loss_average))\n",
        "    loss_training.append(loss_average)\n",
        "\n",
        "\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/6000 (0%)]\t Loss: 2.310048\n",
            "Train Epoch: 0 [640/6000 (11%)]\t Loss: 2.180669\n",
            "Train Epoch: 0 [1280/6000 (21%)]\t Loss: 2.076494\n",
            "Train Epoch: 0 [1920/6000 (32%)]\t Loss: 1.914222\n",
            "Train Epoch: 0 [2560/6000 (43%)]\t Loss: 1.473135\n",
            "Train Epoch: 0 [3200/6000 (53%)]\t Loss: 1.440408\n",
            "Train Epoch: 0 [3840/6000 (64%)]\t Loss: 1.506732\n",
            "Train Epoch: 0 [4480/6000 (74%)]\t Loss: 1.695611\n",
            "Train Epoch: 0 [5120/6000 (85%)]\t Loss: 1.596977\n",
            "Train Epoch: 0 [5760/6000 (96%)]\t Loss: 1.495833\n",
            "Train Epoch: 1 [0/6000 (0%)]\t Loss: 1.385204\n",
            "Train Epoch: 1 [640/6000 (11%)]\t Loss: 1.309572\n",
            "Train Epoch: 1 [1280/6000 (21%)]\t Loss: 1.645228\n",
            "Train Epoch: 1 [1920/6000 (32%)]\t Loss: 1.491972\n",
            "Train Epoch: 1 [2560/6000 (43%)]\t Loss: 1.754777\n",
            "Train Epoch: 1 [3200/6000 (53%)]\t Loss: 1.202806\n",
            "Train Epoch: 1 [3840/6000 (64%)]\t Loss: 1.349053\n",
            "Train Epoch: 1 [4480/6000 (74%)]\t Loss: 1.451753\n",
            "Train Epoch: 1 [5120/6000 (85%)]\t Loss: 1.477899\n",
            "Train Epoch: 1 [5760/6000 (96%)]\t Loss: 1.202051\n",
            "Train Epoch: 2 [0/6000 (0%)]\t Loss: 1.330311\n",
            "Train Epoch: 2 [640/6000 (11%)]\t Loss: 1.205875\n",
            "Train Epoch: 2 [1280/6000 (21%)]\t Loss: 1.290698\n",
            "Train Epoch: 2 [1920/6000 (32%)]\t Loss: 1.362623\n",
            "Train Epoch: 2 [2560/6000 (43%)]\t Loss: 1.648421\n",
            "Train Epoch: 2 [3200/6000 (53%)]\t Loss: 1.274427\n",
            "Train Epoch: 2 [3840/6000 (64%)]\t Loss: 1.434572\n",
            "Train Epoch: 2 [4480/6000 (74%)]\t Loss: 1.344013\n",
            "Train Epoch: 2 [5120/6000 (85%)]\t Loss: 1.351717\n",
            "Train Epoch: 2 [5760/6000 (96%)]\t Loss: 1.173516\n",
            "Train Epoch: 3 [0/6000 (0%)]\t Loss: 1.229043\n",
            "Train Epoch: 3 [640/6000 (11%)]\t Loss: 1.353593\n",
            "Train Epoch: 3 [1280/6000 (21%)]\t Loss: 1.049697\n",
            "Train Epoch: 3 [1920/6000 (32%)]\t Loss: 1.013428\n",
            "Train Epoch: 3 [2560/6000 (43%)]\t Loss: 1.534018\n",
            "Train Epoch: 3 [3200/6000 (53%)]\t Loss: 1.341799\n",
            "Train Epoch: 3 [3840/6000 (64%)]\t Loss: 1.063922\n",
            "Train Epoch: 3 [4480/6000 (74%)]\t Loss: 1.188717\n",
            "Train Epoch: 3 [5120/6000 (85%)]\t Loss: 1.228460\n",
            "Train Epoch: 3 [5760/6000 (96%)]\t Loss: 1.003460\n",
            "Train Epoch: 4 [0/6000 (0%)]\t Loss: 1.092380\n",
            "Train Epoch: 4 [640/6000 (11%)]\t Loss: 1.288453\n",
            "Train Epoch: 4 [1280/6000 (21%)]\t Loss: 1.068241\n",
            "Train Epoch: 4 [1920/6000 (32%)]\t Loss: 1.243502\n",
            "Train Epoch: 4 [2560/6000 (43%)]\t Loss: 1.217734\n",
            "Train Epoch: 4 [3200/6000 (53%)]\t Loss: 1.322437\n",
            "Train Epoch: 4 [3840/6000 (64%)]\t Loss: 1.363339\n",
            "Train Epoch: 4 [4480/6000 (74%)]\t Loss: 1.261659\n",
            "Train Epoch: 4 [5120/6000 (85%)]\t Loss: 1.286744\n",
            "Train Epoch: 4 [5760/6000 (96%)]\t Loss: 1.173301\n",
            "Train Epoch: 5 [0/6000 (0%)]\t Loss: 0.971505\n",
            "Train Epoch: 5 [640/6000 (11%)]\t Loss: 1.587573\n",
            "Train Epoch: 5 [1280/6000 (21%)]\t Loss: 1.153166\n",
            "Train Epoch: 5 [1920/6000 (32%)]\t Loss: 1.429556\n",
            "Train Epoch: 5 [2560/6000 (43%)]\t Loss: 1.269406\n",
            "Train Epoch: 5 [3200/6000 (53%)]\t Loss: 1.430109\n",
            "Train Epoch: 5 [3840/6000 (64%)]\t Loss: 1.100412\n",
            "Train Epoch: 5 [4480/6000 (74%)]\t Loss: 1.110586\n",
            "Train Epoch: 5 [5120/6000 (85%)]\t Loss: 1.168730\n",
            "Train Epoch: 5 [5760/6000 (96%)]\t Loss: 1.083224\n",
            "Train Epoch: 6 [0/6000 (0%)]\t Loss: 1.174142\n",
            "Train Epoch: 6 [640/6000 (11%)]\t Loss: 1.231636\n",
            "Train Epoch: 6 [1280/6000 (21%)]\t Loss: 1.012239\n",
            "Train Epoch: 6 [1920/6000 (32%)]\t Loss: 1.554622\n",
            "Train Epoch: 6 [2560/6000 (43%)]\t Loss: 1.356817\n",
            "Train Epoch: 6 [3200/6000 (53%)]\t Loss: 1.242132\n",
            "Train Epoch: 6 [3840/6000 (64%)]\t Loss: 0.940434\n",
            "Train Epoch: 6 [4480/6000 (74%)]\t Loss: 1.362273\n",
            "Train Epoch: 6 [5120/6000 (85%)]\t Loss: 1.006721\n",
            "Train Epoch: 6 [5760/6000 (96%)]\t Loss: 1.059796\n",
            "Train Epoch: 7 [0/6000 (0%)]\t Loss: 1.314145\n",
            "Train Epoch: 7 [640/6000 (11%)]\t Loss: 1.250902\n",
            "Train Epoch: 7 [1280/6000 (21%)]\t Loss: 1.315252\n",
            "Train Epoch: 7 [1920/6000 (32%)]\t Loss: 1.284418\n",
            "Train Epoch: 7 [2560/6000 (43%)]\t Loss: 1.264407\n",
            "Train Epoch: 7 [3200/6000 (53%)]\t Loss: 1.021614\n",
            "Train Epoch: 7 [3840/6000 (64%)]\t Loss: 1.124568\n",
            "Train Epoch: 7 [4480/6000 (74%)]\t Loss: 1.433807\n",
            "Train Epoch: 7 [5120/6000 (85%)]\t Loss: 1.180616\n",
            "Train Epoch: 7 [5760/6000 (96%)]\t Loss: 1.362107\n",
            "Train Epoch: 8 [0/6000 (0%)]\t Loss: 1.201639\n",
            "Train Epoch: 8 [640/6000 (11%)]\t Loss: 1.095709\n",
            "Train Epoch: 8 [1280/6000 (21%)]\t Loss: 1.085712\n",
            "Train Epoch: 8 [1920/6000 (32%)]\t Loss: 1.332138\n",
            "Train Epoch: 8 [2560/6000 (43%)]\t Loss: 1.203944\n",
            "Train Epoch: 8 [3200/6000 (53%)]\t Loss: 1.243665\n",
            "Train Epoch: 8 [3840/6000 (64%)]\t Loss: 1.074836\n",
            "Train Epoch: 8 [4480/6000 (74%)]\t Loss: 1.264108\n",
            "Train Epoch: 8 [5120/6000 (85%)]\t Loss: 1.387579\n",
            "Train Epoch: 8 [5760/6000 (96%)]\t Loss: 1.450596\n",
            "Train Epoch: 9 [0/6000 (0%)]\t Loss: 1.054601\n",
            "Train Epoch: 9 [640/6000 (11%)]\t Loss: 1.043036\n",
            "Train Epoch: 9 [1280/6000 (21%)]\t Loss: 1.484640\n",
            "Train Epoch: 9 [1920/6000 (32%)]\t Loss: 0.904494\n",
            "Train Epoch: 9 [2560/6000 (43%)]\t Loss: 1.337515\n",
            "Train Epoch: 9 [3200/6000 (53%)]\t Loss: 1.148579\n",
            "Train Epoch: 9 [3840/6000 (64%)]\t Loss: 1.214800\n",
            "Train Epoch: 9 [4480/6000 (74%)]\t Loss: 1.061026\n",
            "Train Epoch: 9 [5120/6000 (85%)]\t Loss: 1.323443\n",
            "Train Epoch: 9 [5760/6000 (96%)]\t Loss: 1.395101\n",
            "Iteration   0, Average Loss 1.31\n",
            "Train Epoch: 0 [0/6000 (0%)]\t Loss: 1.052525\n",
            "Train Epoch: 0 [640/6000 (11%)]\t Loss: 1.279530\n",
            "Train Epoch: 0 [1280/6000 (21%)]\t Loss: 0.757230\n",
            "Train Epoch: 0 [1920/6000 (32%)]\t Loss: 1.106241\n",
            "Train Epoch: 0 [2560/6000 (43%)]\t Loss: 1.316089\n",
            "Train Epoch: 0 [3200/6000 (53%)]\t Loss: 1.155205\n",
            "Train Epoch: 0 [3840/6000 (64%)]\t Loss: 1.130597\n",
            "Train Epoch: 0 [4480/6000 (74%)]\t Loss: 1.324592\n",
            "Train Epoch: 0 [5120/6000 (85%)]\t Loss: 1.188204\n",
            "Train Epoch: 0 [5760/6000 (96%)]\t Loss: 1.206263\n",
            "Train Epoch: 1 [0/6000 (0%)]\t Loss: 1.600488\n",
            "Train Epoch: 1 [640/6000 (11%)]\t Loss: 1.191533\n",
            "Train Epoch: 1 [1280/6000 (21%)]\t Loss: 1.131012\n",
            "Train Epoch: 1 [1920/6000 (32%)]\t Loss: 1.193184\n",
            "Train Epoch: 1 [2560/6000 (43%)]\t Loss: 1.075227\n",
            "Train Epoch: 1 [3200/6000 (53%)]\t Loss: 0.973910\n",
            "Train Epoch: 1 [3840/6000 (64%)]\t Loss: 0.998047\n",
            "Train Epoch: 1 [4480/6000 (74%)]\t Loss: 1.317924\n",
            "Train Epoch: 1 [5120/6000 (85%)]\t Loss: 1.124582\n",
            "Train Epoch: 1 [5760/6000 (96%)]\t Loss: 1.440350\n",
            "Train Epoch: 2 [0/6000 (0%)]\t Loss: 1.452790\n",
            "Train Epoch: 2 [640/6000 (11%)]\t Loss: 1.513234\n",
            "Train Epoch: 2 [1280/6000 (21%)]\t Loss: 1.503269\n",
            "Train Epoch: 2 [1920/6000 (32%)]\t Loss: 0.886320\n",
            "Train Epoch: 2 [2560/6000 (43%)]\t Loss: 1.105117\n",
            "Train Epoch: 2 [3200/6000 (53%)]\t Loss: 1.107193\n",
            "Train Epoch: 2 [3840/6000 (64%)]\t Loss: 1.240613\n",
            "Train Epoch: 2 [4480/6000 (74%)]\t Loss: 0.932381\n",
            "Train Epoch: 2 [5120/6000 (85%)]\t Loss: 1.157349\n",
            "Train Epoch: 2 [5760/6000 (96%)]\t Loss: 1.096173\n",
            "Train Epoch: 3 [0/6000 (0%)]\t Loss: 1.013867\n",
            "Train Epoch: 3 [640/6000 (11%)]\t Loss: 1.439293\n",
            "Train Epoch: 3 [1280/6000 (21%)]\t Loss: 1.298573\n",
            "Train Epoch: 3 [1920/6000 (32%)]\t Loss: 1.399159\n",
            "Train Epoch: 3 [2560/6000 (43%)]\t Loss: 1.204127\n",
            "Train Epoch: 3 [3200/6000 (53%)]\t Loss: 1.194958\n",
            "Train Epoch: 3 [3840/6000 (64%)]\t Loss: 1.127360\n",
            "Train Epoch: 3 [4480/6000 (74%)]\t Loss: 0.998492\n",
            "Train Epoch: 3 [5120/6000 (85%)]\t Loss: 1.254153\n",
            "Train Epoch: 3 [5760/6000 (96%)]\t Loss: 1.047766\n",
            "Train Epoch: 4 [0/6000 (0%)]\t Loss: 1.011778\n",
            "Train Epoch: 4 [640/6000 (11%)]\t Loss: 1.218778\n",
            "Train Epoch: 4 [1280/6000 (21%)]\t Loss: 1.461987\n",
            "Train Epoch: 4 [1920/6000 (32%)]\t Loss: 1.223714\n",
            "Train Epoch: 4 [2560/6000 (43%)]\t Loss: 1.046753\n",
            "Train Epoch: 4 [3200/6000 (53%)]\t Loss: 1.358476\n",
            "Train Epoch: 4 [3840/6000 (64%)]\t Loss: 1.043362\n",
            "Train Epoch: 4 [4480/6000 (74%)]\t Loss: 1.065911\n",
            "Train Epoch: 4 [5120/6000 (85%)]\t Loss: 1.141214\n",
            "Train Epoch: 4 [5760/6000 (96%)]\t Loss: 1.106870\n",
            "Train Epoch: 5 [0/6000 (0%)]\t Loss: 0.884450\n",
            "Train Epoch: 5 [640/6000 (11%)]\t Loss: 1.149576\n",
            "Train Epoch: 5 [1280/6000 (21%)]\t Loss: 1.283253\n",
            "Train Epoch: 5 [1920/6000 (32%)]\t Loss: 1.174476\n",
            "Train Epoch: 5 [2560/6000 (43%)]\t Loss: 1.202100\n",
            "Train Epoch: 5 [3200/6000 (53%)]\t Loss: 0.946285\n",
            "Train Epoch: 5 [3840/6000 (64%)]\t Loss: 1.206451\n",
            "Train Epoch: 5 [4480/6000 (74%)]\t Loss: 1.372709\n",
            "Train Epoch: 5 [5120/6000 (85%)]\t Loss: 1.352388\n",
            "Train Epoch: 5 [5760/6000 (96%)]\t Loss: 1.133189\n",
            "Train Epoch: 6 [0/6000 (0%)]\t Loss: 1.210156\n",
            "Train Epoch: 6 [640/6000 (11%)]\t Loss: 1.273302\n",
            "Train Epoch: 6 [1280/6000 (21%)]\t Loss: 1.136542\n",
            "Train Epoch: 6 [1920/6000 (32%)]\t Loss: 1.366632\n",
            "Train Epoch: 6 [2560/6000 (43%)]\t Loss: 1.161736\n",
            "Train Epoch: 6 [3200/6000 (53%)]\t Loss: 1.362892\n",
            "Train Epoch: 6 [3840/6000 (64%)]\t Loss: 1.171209\n",
            "Train Epoch: 6 [4480/6000 (74%)]\t Loss: 1.011618\n",
            "Train Epoch: 6 [5120/6000 (85%)]\t Loss: 1.008320\n",
            "Train Epoch: 6 [5760/6000 (96%)]\t Loss: 1.540410\n",
            "Train Epoch: 7 [0/6000 (0%)]\t Loss: 1.277571\n",
            "Train Epoch: 7 [640/6000 (11%)]\t Loss: 0.963809\n",
            "Train Epoch: 7 [1280/6000 (21%)]\t Loss: 1.157057\n",
            "Train Epoch: 7 [1920/6000 (32%)]\t Loss: 0.935893\n",
            "Train Epoch: 7 [2560/6000 (43%)]\t Loss: 1.001952\n",
            "Train Epoch: 7 [3200/6000 (53%)]\t Loss: 1.072854\n",
            "Train Epoch: 7 [3840/6000 (64%)]\t Loss: 1.278373\n",
            "Train Epoch: 7 [4480/6000 (74%)]\t Loss: 1.325652\n",
            "Train Epoch: 7 [5120/6000 (85%)]\t Loss: 1.149007\n",
            "Train Epoch: 7 [5760/6000 (96%)]\t Loss: 1.235665\n",
            "Train Epoch: 8 [0/6000 (0%)]\t Loss: 1.097283\n",
            "Train Epoch: 8 [640/6000 (11%)]\t Loss: 1.330759\n",
            "Train Epoch: 8 [1280/6000 (21%)]\t Loss: 1.157016\n",
            "Train Epoch: 8 [1920/6000 (32%)]\t Loss: 1.250872\n",
            "Train Epoch: 8 [2560/6000 (43%)]\t Loss: 1.033761\n",
            "Train Epoch: 8 [3200/6000 (53%)]\t Loss: 0.941715\n",
            "Train Epoch: 8 [3840/6000 (64%)]\t Loss: 1.041750\n",
            "Train Epoch: 8 [4480/6000 (74%)]\t Loss: 1.153487\n",
            "Train Epoch: 8 [5120/6000 (85%)]\t Loss: 0.979608\n",
            "Train Epoch: 8 [5760/6000 (96%)]\t Loss: 1.431631\n",
            "Train Epoch: 9 [0/6000 (0%)]\t Loss: 1.450207\n",
            "Train Epoch: 9 [640/6000 (11%)]\t Loss: 1.136557\n",
            "Train Epoch: 9 [1280/6000 (21%)]\t Loss: 0.882189\n",
            "Train Epoch: 9 [1920/6000 (32%)]\t Loss: 1.017381\n",
            "Train Epoch: 9 [2560/6000 (43%)]\t Loss: 1.185335\n",
            "Train Epoch: 9 [3200/6000 (53%)]\t Loss: 1.233659\n",
            "Train Epoch: 9 [3840/6000 (64%)]\t Loss: 0.899946\n",
            "Train Epoch: 9 [4480/6000 (74%)]\t Loss: 1.315759\n",
            "Train Epoch: 9 [5120/6000 (85%)]\t Loss: 1.077983\n",
            "Train Epoch: 9 [5760/6000 (96%)]\t Loss: 1.172780\n",
            "Iteration   1, Average Loss 1.17\n",
            "Train Epoch: 0 [0/6000 (0%)]\t Loss: 1.127285\n",
            "Train Epoch: 0 [640/6000 (11%)]\t Loss: 1.157895\n",
            "Train Epoch: 0 [1280/6000 (21%)]\t Loss: 1.128042\n",
            "Train Epoch: 0 [1920/6000 (32%)]\t Loss: 1.261012\n",
            "Train Epoch: 0 [2560/6000 (43%)]\t Loss: 1.306827\n",
            "Train Epoch: 0 [3200/6000 (53%)]\t Loss: 0.887211\n",
            "Train Epoch: 0 [3840/6000 (64%)]\t Loss: 1.123633\n",
            "Train Epoch: 0 [4480/6000 (74%)]\t Loss: 1.379204\n",
            "Train Epoch: 0 [5120/6000 (85%)]\t Loss: 1.288671\n",
            "Train Epoch: 0 [5760/6000 (96%)]\t Loss: 1.216195\n",
            "Train Epoch: 1 [0/6000 (0%)]\t Loss: 0.882075\n",
            "Train Epoch: 1 [640/6000 (11%)]\t Loss: 1.084630\n",
            "Train Epoch: 1 [1280/6000 (21%)]\t Loss: 1.091299\n",
            "Train Epoch: 1 [1920/6000 (32%)]\t Loss: 1.562224\n",
            "Train Epoch: 1 [2560/6000 (43%)]\t Loss: 1.142595\n",
            "Train Epoch: 1 [3200/6000 (53%)]\t Loss: 1.172432\n",
            "Train Epoch: 1 [3840/6000 (64%)]\t Loss: 1.186113\n",
            "Train Epoch: 1 [4480/6000 (74%)]\t Loss: 1.448056\n",
            "Train Epoch: 1 [5120/6000 (85%)]\t Loss: 1.182202\n",
            "Train Epoch: 1 [5760/6000 (96%)]\t Loss: 1.145810\n",
            "Train Epoch: 2 [0/6000 (0%)]\t Loss: 1.279469\n",
            "Train Epoch: 2 [640/6000 (11%)]\t Loss: 1.270902\n",
            "Train Epoch: 2 [1280/6000 (21%)]\t Loss: 1.066194\n",
            "Train Epoch: 2 [1920/6000 (32%)]\t Loss: 1.294257\n",
            "Train Epoch: 2 [2560/6000 (43%)]\t Loss: 1.208718\n",
            "Train Epoch: 2 [3200/6000 (53%)]\t Loss: 1.114846\n",
            "Train Epoch: 2 [3840/6000 (64%)]\t Loss: 0.904939\n",
            "Train Epoch: 2 [4480/6000 (74%)]\t Loss: 1.237809\n",
            "Train Epoch: 2 [5120/6000 (85%)]\t Loss: 1.405324\n",
            "Train Epoch: 2 [5760/6000 (96%)]\t Loss: 1.041728\n",
            "Train Epoch: 3 [0/6000 (0%)]\t Loss: 1.209151\n",
            "Train Epoch: 3 [640/6000 (11%)]\t Loss: 0.841980\n",
            "Train Epoch: 3 [1280/6000 (21%)]\t Loss: 1.080087\n",
            "Train Epoch: 3 [1920/6000 (32%)]\t Loss: 0.993562\n",
            "Train Epoch: 3 [2560/6000 (43%)]\t Loss: 0.931295\n",
            "Train Epoch: 3 [3200/6000 (53%)]\t Loss: 1.311768\n",
            "Train Epoch: 3 [3840/6000 (64%)]\t Loss: 1.206915\n",
            "Train Epoch: 3 [4480/6000 (74%)]\t Loss: 1.317988\n",
            "Train Epoch: 3 [5120/6000 (85%)]\t Loss: 1.389591\n",
            "Train Epoch: 3 [5760/6000 (96%)]\t Loss: 1.245491\n",
            "Train Epoch: 4 [0/6000 (0%)]\t Loss: 1.309038\n",
            "Train Epoch: 4 [640/6000 (11%)]\t Loss: 1.264836\n",
            "Train Epoch: 4 [1280/6000 (21%)]\t Loss: 1.224396\n",
            "Train Epoch: 4 [1920/6000 (32%)]\t Loss: 1.185413\n",
            "Train Epoch: 4 [2560/6000 (43%)]\t Loss: 1.113004\n",
            "Train Epoch: 4 [3200/6000 (53%)]\t Loss: 1.088851\n",
            "Train Epoch: 4 [3840/6000 (64%)]\t Loss: 1.120199\n",
            "Train Epoch: 4 [4480/6000 (74%)]\t Loss: 1.039455\n",
            "Train Epoch: 4 [5120/6000 (85%)]\t Loss: 1.182573\n",
            "Train Epoch: 4 [5760/6000 (96%)]\t Loss: 1.500745\n",
            "Train Epoch: 5 [0/6000 (0%)]\t Loss: 1.235204\n",
            "Train Epoch: 5 [640/6000 (11%)]\t Loss: 1.094585\n",
            "Train Epoch: 5 [1280/6000 (21%)]\t Loss: 1.416448\n",
            "Train Epoch: 5 [1920/6000 (32%)]\t Loss: 0.915595\n",
            "Train Epoch: 5 [2560/6000 (43%)]\t Loss: 1.034503\n",
            "Train Epoch: 5 [3200/6000 (53%)]\t Loss: 1.479872\n",
            "Train Epoch: 5 [3840/6000 (64%)]\t Loss: 1.304078\n",
            "Train Epoch: 5 [4480/6000 (74%)]\t Loss: 1.212887\n",
            "Train Epoch: 5 [5120/6000 (85%)]\t Loss: 1.072786\n",
            "Train Epoch: 5 [5760/6000 (96%)]\t Loss: 1.270311\n",
            "Train Epoch: 6 [0/6000 (0%)]\t Loss: 1.392459\n",
            "Train Epoch: 6 [640/6000 (11%)]\t Loss: 1.013884\n",
            "Train Epoch: 6 [1280/6000 (21%)]\t Loss: 1.028694\n",
            "Train Epoch: 6 [1920/6000 (32%)]\t Loss: 1.706116\n",
            "Train Epoch: 6 [2560/6000 (43%)]\t Loss: 1.118201\n",
            "Train Epoch: 6 [3200/6000 (53%)]\t Loss: 0.870333\n",
            "Train Epoch: 6 [3840/6000 (64%)]\t Loss: 1.083316\n",
            "Train Epoch: 6 [4480/6000 (74%)]\t Loss: 1.263868\n",
            "Train Epoch: 6 [5120/6000 (85%)]\t Loss: 1.029867\n",
            "Train Epoch: 6 [5760/6000 (96%)]\t Loss: 1.299084\n",
            "Train Epoch: 7 [0/6000 (0%)]\t Loss: 1.211505\n",
            "Train Epoch: 7 [640/6000 (11%)]\t Loss: 1.261958\n",
            "Train Epoch: 7 [1280/6000 (21%)]\t Loss: 1.085275\n",
            "Train Epoch: 7 [1920/6000 (32%)]\t Loss: 0.883026\n",
            "Train Epoch: 7 [2560/6000 (43%)]\t Loss: 1.366277\n",
            "Train Epoch: 7 [3200/6000 (53%)]\t Loss: 0.719379\n",
            "Train Epoch: 7 [3840/6000 (64%)]\t Loss: 1.134144\n",
            "Train Epoch: 7 [4480/6000 (74%)]\t Loss: 0.881446\n",
            "Train Epoch: 7 [5120/6000 (85%)]\t Loss: 0.958584\n",
            "Train Epoch: 7 [5760/6000 (96%)]\t Loss: 1.213483\n",
            "Train Epoch: 8 [0/6000 (0%)]\t Loss: 0.971383\n",
            "Train Epoch: 8 [640/6000 (11%)]\t Loss: 0.815844\n",
            "Train Epoch: 8 [1280/6000 (21%)]\t Loss: 1.037217\n",
            "Train Epoch: 8 [1920/6000 (32%)]\t Loss: 0.810547\n",
            "Train Epoch: 8 [2560/6000 (43%)]\t Loss: 0.821529\n",
            "Train Epoch: 8 [3200/6000 (53%)]\t Loss: 1.098009\n",
            "Train Epoch: 8 [3840/6000 (64%)]\t Loss: 0.809214\n",
            "Train Epoch: 8 [4480/6000 (74%)]\t Loss: 0.922870\n",
            "Train Epoch: 8 [5120/6000 (85%)]\t Loss: 1.088440\n",
            "Train Epoch: 8 [5760/6000 (96%)]\t Loss: 0.880457\n",
            "Train Epoch: 9 [0/6000 (0%)]\t Loss: 0.890188\n",
            "Train Epoch: 9 [640/6000 (11%)]\t Loss: 0.822194\n",
            "Train Epoch: 9 [1280/6000 (21%)]\t Loss: 1.110439\n",
            "Train Epoch: 9 [1920/6000 (32%)]\t Loss: 1.003961\n",
            "Train Epoch: 9 [2560/6000 (43%)]\t Loss: 0.950532\n",
            "Train Epoch: 9 [3200/6000 (53%)]\t Loss: 0.891444\n",
            "Train Epoch: 9 [3840/6000 (64%)]\t Loss: 0.902781\n",
            "Train Epoch: 9 [4480/6000 (74%)]\t Loss: 1.115159\n",
            "Train Epoch: 9 [5120/6000 (85%)]\t Loss: 1.116681\n",
            "Train Epoch: 9 [5760/6000 (96%)]\t Loss: 0.889370\n",
            "Iteration   2, Average Loss 1.12\n",
            "Train Epoch: 0 [0/6000 (0%)]\t Loss: 1.064137\n",
            "Train Epoch: 0 [640/6000 (11%)]\t Loss: 1.032307\n",
            "Train Epoch: 0 [1280/6000 (21%)]\t Loss: 1.060386\n",
            "Train Epoch: 0 [1920/6000 (32%)]\t Loss: 1.186352\n",
            "Train Epoch: 0 [2560/6000 (43%)]\t Loss: 0.868795\n",
            "Train Epoch: 0 [3200/6000 (53%)]\t Loss: 1.053148\n",
            "Train Epoch: 0 [3840/6000 (64%)]\t Loss: 1.038854\n",
            "Train Epoch: 0 [4480/6000 (74%)]\t Loss: 1.024545\n",
            "Train Epoch: 0 [5120/6000 (85%)]\t Loss: 1.164965\n",
            "Train Epoch: 0 [5760/6000 (96%)]\t Loss: 1.012305\n",
            "Train Epoch: 1 [0/6000 (0%)]\t Loss: 0.869869\n",
            "Train Epoch: 1 [640/6000 (11%)]\t Loss: 0.869484\n",
            "Train Epoch: 1 [1280/6000 (21%)]\t Loss: 1.102404\n",
            "Train Epoch: 1 [1920/6000 (32%)]\t Loss: 0.996808\n",
            "Train Epoch: 1 [2560/6000 (43%)]\t Loss: 1.108872\n",
            "Train Epoch: 1 [3200/6000 (53%)]\t Loss: 0.981043\n",
            "Train Epoch: 1 [3840/6000 (64%)]\t Loss: 0.774387\n",
            "Train Epoch: 1 [4480/6000 (74%)]\t Loss: 0.975927\n",
            "Train Epoch: 1 [5120/6000 (85%)]\t Loss: 1.156363\n",
            "Train Epoch: 1 [5760/6000 (96%)]\t Loss: 0.962340\n",
            "Train Epoch: 2 [0/6000 (0%)]\t Loss: 0.900250\n",
            "Train Epoch: 2 [640/6000 (11%)]\t Loss: 0.918472\n",
            "Train Epoch: 2 [1280/6000 (21%)]\t Loss: 0.928033\n",
            "Train Epoch: 2 [1920/6000 (32%)]\t Loss: 0.690975\n",
            "Train Epoch: 2 [2560/6000 (43%)]\t Loss: 0.995460\n",
            "Train Epoch: 2 [3200/6000 (53%)]\t Loss: 0.919888\n",
            "Train Epoch: 2 [3840/6000 (64%)]\t Loss: 0.801158\n",
            "Train Epoch: 2 [4480/6000 (74%)]\t Loss: 1.156865\n",
            "Train Epoch: 2 [5120/6000 (85%)]\t Loss: 0.706404\n",
            "Train Epoch: 2 [5760/6000 (96%)]\t Loss: 1.077871\n",
            "Train Epoch: 3 [0/6000 (0%)]\t Loss: 0.912298\n",
            "Train Epoch: 3 [640/6000 (11%)]\t Loss: 0.900562\n",
            "Train Epoch: 3 [1280/6000 (21%)]\t Loss: 0.852778\n",
            "Train Epoch: 3 [1920/6000 (32%)]\t Loss: 0.884026\n",
            "Train Epoch: 3 [2560/6000 (43%)]\t Loss: 0.816368\n",
            "Train Epoch: 3 [3200/6000 (53%)]\t Loss: 0.863852\n",
            "Train Epoch: 3 [3840/6000 (64%)]\t Loss: 0.921857\n",
            "Train Epoch: 3 [4480/6000 (74%)]\t Loss: 0.619934\n",
            "Train Epoch: 3 [5120/6000 (85%)]\t Loss: 1.156930\n",
            "Train Epoch: 3 [5760/6000 (96%)]\t Loss: 1.299603\n",
            "Train Epoch: 4 [0/6000 (0%)]\t Loss: 0.905721\n",
            "Train Epoch: 4 [640/6000 (11%)]\t Loss: 1.051678\n",
            "Train Epoch: 4 [1280/6000 (21%)]\t Loss: 0.933080\n",
            "Train Epoch: 4 [1920/6000 (32%)]\t Loss: 1.110767\n",
            "Train Epoch: 4 [2560/6000 (43%)]\t Loss: 0.890534\n",
            "Train Epoch: 4 [3200/6000 (53%)]\t Loss: 0.853756\n",
            "Train Epoch: 4 [3840/6000 (64%)]\t Loss: 1.021712\n",
            "Train Epoch: 4 [4480/6000 (74%)]\t Loss: 0.958039\n",
            "Train Epoch: 4 [5120/6000 (85%)]\t Loss: 1.092828\n",
            "Train Epoch: 4 [5760/6000 (96%)]\t Loss: 0.805882\n",
            "Train Epoch: 5 [0/6000 (0%)]\t Loss: 1.023660\n",
            "Train Epoch: 5 [640/6000 (11%)]\t Loss: 0.887434\n",
            "Train Epoch: 5 [1280/6000 (21%)]\t Loss: 1.038345\n",
            "Train Epoch: 5 [1920/6000 (32%)]\t Loss: 0.865047\n",
            "Train Epoch: 5 [2560/6000 (43%)]\t Loss: 0.796170\n",
            "Train Epoch: 5 [3200/6000 (53%)]\t Loss: 0.920812\n",
            "Train Epoch: 5 [3840/6000 (64%)]\t Loss: 1.340204\n",
            "Train Epoch: 5 [4480/6000 (74%)]\t Loss: 1.174479\n",
            "Train Epoch: 5 [5120/6000 (85%)]\t Loss: 0.836630\n",
            "Train Epoch: 5 [5760/6000 (96%)]\t Loss: 1.163766\n",
            "Train Epoch: 6 [0/6000 (0%)]\t Loss: 0.879293\n",
            "Train Epoch: 6 [640/6000 (11%)]\t Loss: 0.692226\n",
            "Train Epoch: 6 [1280/6000 (21%)]\t Loss: 1.042064\n",
            "Train Epoch: 6 [1920/6000 (32%)]\t Loss: 1.067512\n",
            "Train Epoch: 6 [2560/6000 (43%)]\t Loss: 0.943354\n",
            "Train Epoch: 6 [3200/6000 (53%)]\t Loss: 0.844294\n",
            "Train Epoch: 6 [3840/6000 (64%)]\t Loss: 1.010665\n",
            "Train Epoch: 6 [4480/6000 (74%)]\t Loss: 1.017330\n",
            "Train Epoch: 6 [5120/6000 (85%)]\t Loss: 0.785814\n",
            "Train Epoch: 6 [5760/6000 (96%)]\t Loss: 1.053548\n",
            "Train Epoch: 7 [0/6000 (0%)]\t Loss: 0.809082\n",
            "Train Epoch: 7 [640/6000 (11%)]\t Loss: 1.041784\n",
            "Train Epoch: 7 [1280/6000 (21%)]\t Loss: 0.924298\n",
            "Train Epoch: 7 [1920/6000 (32%)]\t Loss: 0.990199\n",
            "Train Epoch: 7 [2560/6000 (43%)]\t Loss: 1.126783\n",
            "Train Epoch: 7 [3200/6000 (53%)]\t Loss: 1.099264\n",
            "Train Epoch: 7 [3840/6000 (64%)]\t Loss: 0.891234\n",
            "Train Epoch: 7 [4480/6000 (74%)]\t Loss: 1.120611\n",
            "Train Epoch: 7 [5120/6000 (85%)]\t Loss: 1.063136\n",
            "Train Epoch: 7 [5760/6000 (96%)]\t Loss: 0.854527\n",
            "Train Epoch: 8 [0/6000 (0%)]\t Loss: 0.902245\n",
            "Train Epoch: 8 [640/6000 (11%)]\t Loss: 1.126067\n",
            "Train Epoch: 8 [1280/6000 (21%)]\t Loss: 0.920217\n",
            "Train Epoch: 8 [1920/6000 (32%)]\t Loss: 1.117874\n",
            "Train Epoch: 8 [2560/6000 (43%)]\t Loss: 0.790505\n",
            "Train Epoch: 8 [3200/6000 (53%)]\t Loss: 0.819952\n",
            "Train Epoch: 8 [3840/6000 (64%)]\t Loss: 1.127746\n",
            "Train Epoch: 8 [4480/6000 (74%)]\t Loss: 1.118866\n",
            "Train Epoch: 8 [5120/6000 (85%)]\t Loss: 1.054355\n",
            "Train Epoch: 8 [5760/6000 (96%)]\t Loss: 1.079714\n",
            "Train Epoch: 9 [0/6000 (0%)]\t Loss: 0.882365\n",
            "Train Epoch: 9 [640/6000 (11%)]\t Loss: 0.788698\n",
            "Train Epoch: 9 [1280/6000 (21%)]\t Loss: 0.839080\n",
            "Train Epoch: 9 [1920/6000 (32%)]\t Loss: 0.798531\n",
            "Train Epoch: 9 [2560/6000 (43%)]\t Loss: 1.208692\n",
            "Train Epoch: 9 [3200/6000 (53%)]\t Loss: 0.980227\n",
            "Train Epoch: 9 [3840/6000 (64%)]\t Loss: 0.816957\n",
            "Train Epoch: 9 [4480/6000 (74%)]\t Loss: 1.154248\n",
            "Train Epoch: 9 [5120/6000 (85%)]\t Loss: 0.955541\n",
            "Train Epoch: 9 [5760/6000 (96%)]\t Loss: 0.965839\n",
            "Iteration   3, Average Loss 0.96\n",
            "Train Epoch: 0 [0/6000 (0%)]\t Loss: 1.227754\n",
            "Train Epoch: 0 [640/6000 (11%)]\t Loss: 0.881362\n",
            "Train Epoch: 0 [1280/6000 (21%)]\t Loss: 0.940879\n",
            "Train Epoch: 0 [1920/6000 (32%)]\t Loss: 0.877498\n",
            "Train Epoch: 0 [2560/6000 (43%)]\t Loss: 1.019003\n",
            "Train Epoch: 0 [3200/6000 (53%)]\t Loss: 1.010770\n",
            "Train Epoch: 0 [3840/6000 (64%)]\t Loss: 0.635392\n",
            "Train Epoch: 0 [4480/6000 (74%)]\t Loss: 1.063973\n",
            "Train Epoch: 0 [5120/6000 (85%)]\t Loss: 0.878737\n",
            "Train Epoch: 0 [5760/6000 (96%)]\t Loss: 1.148788\n",
            "Train Epoch: 1 [0/6000 (0%)]\t Loss: 0.788057\n",
            "Train Epoch: 1 [640/6000 (11%)]\t Loss: 1.155850\n",
            "Train Epoch: 1 [1280/6000 (21%)]\t Loss: 0.978144\n",
            "Train Epoch: 1 [1920/6000 (32%)]\t Loss: 1.238254\n",
            "Train Epoch: 1 [2560/6000 (43%)]\t Loss: 1.068730\n",
            "Train Epoch: 1 [3200/6000 (53%)]\t Loss: 0.952765\n",
            "Train Epoch: 1 [3840/6000 (64%)]\t Loss: 0.841651\n",
            "Train Epoch: 1 [4480/6000 (74%)]\t Loss: 1.088737\n",
            "Train Epoch: 1 [5120/6000 (85%)]\t Loss: 1.053841\n",
            "Train Epoch: 1 [5760/6000 (96%)]\t Loss: 1.103281\n",
            "Train Epoch: 2 [0/6000 (0%)]\t Loss: 0.909635\n",
            "Train Epoch: 2 [640/6000 (11%)]\t Loss: 0.909267\n",
            "Train Epoch: 2 [1280/6000 (21%)]\t Loss: 0.803207\n",
            "Train Epoch: 2 [1920/6000 (32%)]\t Loss: 1.020030\n",
            "Train Epoch: 2 [2560/6000 (43%)]\t Loss: 1.086845\n",
            "Train Epoch: 2 [3200/6000 (53%)]\t Loss: 1.253983\n",
            "Train Epoch: 2 [3840/6000 (64%)]\t Loss: 0.851138\n",
            "Train Epoch: 2 [4480/6000 (74%)]\t Loss: 1.022164\n",
            "Train Epoch: 2 [5120/6000 (85%)]\t Loss: 1.231885\n",
            "Train Epoch: 2 [5760/6000 (96%)]\t Loss: 0.800938\n",
            "Train Epoch: 3 [0/6000 (0%)]\t Loss: 0.731938\n",
            "Train Epoch: 3 [640/6000 (11%)]\t Loss: 0.932233\n",
            "Train Epoch: 3 [1280/6000 (21%)]\t Loss: 1.271334\n",
            "Train Epoch: 3 [1920/6000 (32%)]\t Loss: 1.023084\n",
            "Train Epoch: 3 [2560/6000 (43%)]\t Loss: 0.763734\n",
            "Train Epoch: 3 [3200/6000 (53%)]\t Loss: 0.843496\n",
            "Train Epoch: 3 [3840/6000 (64%)]\t Loss: 0.814734\n",
            "Train Epoch: 3 [4480/6000 (74%)]\t Loss: 0.826573\n",
            "Train Epoch: 3 [5120/6000 (85%)]\t Loss: 1.029521\n",
            "Train Epoch: 3 [5760/6000 (96%)]\t Loss: 0.975715\n",
            "Train Epoch: 4 [0/6000 (0%)]\t Loss: 0.890489\n",
            "Train Epoch: 4 [640/6000 (11%)]\t Loss: 1.139447\n",
            "Train Epoch: 4 [1280/6000 (21%)]\t Loss: 0.985979\n",
            "Train Epoch: 4 [1920/6000 (32%)]\t Loss: 0.983907\n",
            "Train Epoch: 4 [2560/6000 (43%)]\t Loss: 1.004674\n",
            "Train Epoch: 4 [3200/6000 (53%)]\t Loss: 1.174702\n",
            "Train Epoch: 4 [3840/6000 (64%)]\t Loss: 1.024866\n",
            "Train Epoch: 4 [4480/6000 (74%)]\t Loss: 0.857294\n",
            "Train Epoch: 4 [5120/6000 (85%)]\t Loss: 0.863006\n",
            "Train Epoch: 4 [5760/6000 (96%)]\t Loss: 0.809358\n",
            "Train Epoch: 5 [0/6000 (0%)]\t Loss: 1.081209\n",
            "Train Epoch: 5 [640/6000 (11%)]\t Loss: 0.996156\n",
            "Train Epoch: 5 [1280/6000 (21%)]\t Loss: 0.856471\n",
            "Train Epoch: 5 [1920/6000 (32%)]\t Loss: 1.232221\n",
            "Train Epoch: 5 [2560/6000 (43%)]\t Loss: 1.011730\n",
            "Train Epoch: 5 [3200/6000 (53%)]\t Loss: 0.811504\n",
            "Train Epoch: 5 [3840/6000 (64%)]\t Loss: 1.050837\n",
            "Train Epoch: 5 [4480/6000 (74%)]\t Loss: 0.883311\n",
            "Train Epoch: 5 [5120/6000 (85%)]\t Loss: 0.987548\n",
            "Train Epoch: 5 [5760/6000 (96%)]\t Loss: 0.880628\n",
            "Train Epoch: 6 [0/6000 (0%)]\t Loss: 0.789143\n",
            "Train Epoch: 6 [640/6000 (11%)]\t Loss: 0.817321\n",
            "Train Epoch: 6 [1280/6000 (21%)]\t Loss: 1.278972\n",
            "Train Epoch: 6 [1920/6000 (32%)]\t Loss: 1.164590\n",
            "Train Epoch: 6 [2560/6000 (43%)]\t Loss: 0.976689\n",
            "Train Epoch: 6 [3200/6000 (53%)]\t Loss: 0.939770\n",
            "Train Epoch: 6 [3840/6000 (64%)]\t Loss: 0.912717\n",
            "Train Epoch: 6 [4480/6000 (74%)]\t Loss: 1.055048\n",
            "Train Epoch: 6 [5120/6000 (85%)]\t Loss: 1.008688\n",
            "Train Epoch: 6 [5760/6000 (96%)]\t Loss: 1.018879\n",
            "Train Epoch: 7 [0/6000 (0%)]\t Loss: 1.042207\n",
            "Train Epoch: 7 [640/6000 (11%)]\t Loss: 1.012193\n",
            "Train Epoch: 7 [1280/6000 (21%)]\t Loss: 0.985586\n",
            "Train Epoch: 7 [1920/6000 (32%)]\t Loss: 1.126913\n",
            "Train Epoch: 7 [2560/6000 (43%)]\t Loss: 0.956007\n",
            "Train Epoch: 7 [3200/6000 (53%)]\t Loss: 0.914668\n",
            "Train Epoch: 7 [3840/6000 (64%)]\t Loss: 0.840361\n",
            "Train Epoch: 7 [4480/6000 (74%)]\t Loss: 0.918117\n",
            "Train Epoch: 7 [5120/6000 (85%)]\t Loss: 0.890883\n",
            "Train Epoch: 7 [5760/6000 (96%)]\t Loss: 1.080143\n",
            "Train Epoch: 8 [0/6000 (0%)]\t Loss: 1.191072\n",
            "Train Epoch: 8 [640/6000 (11%)]\t Loss: 1.223283\n",
            "Train Epoch: 8 [1280/6000 (21%)]\t Loss: 0.870252\n",
            "Train Epoch: 8 [1920/6000 (32%)]\t Loss: 0.838805\n",
            "Train Epoch: 8 [2560/6000 (43%)]\t Loss: 0.798220\n",
            "Train Epoch: 8 [3200/6000 (53%)]\t Loss: 1.101552\n",
            "Train Epoch: 8 [3840/6000 (64%)]\t Loss: 0.996142\n",
            "Train Epoch: 8 [4480/6000 (74%)]\t Loss: 0.972509\n",
            "Train Epoch: 8 [5120/6000 (85%)]\t Loss: 0.994413\n",
            "Train Epoch: 8 [5760/6000 (96%)]\t Loss: 0.694217\n",
            "Train Epoch: 9 [0/6000 (0%)]\t Loss: 0.839306\n",
            "Train Epoch: 9 [640/6000 (11%)]\t Loss: 1.051749\n",
            "Train Epoch: 9 [1280/6000 (21%)]\t Loss: 0.983395\n",
            "Train Epoch: 9 [1920/6000 (32%)]\t Loss: 1.108189\n",
            "Train Epoch: 9 [2560/6000 (43%)]\t Loss: 0.978667\n",
            "Train Epoch: 9 [3200/6000 (53%)]\t Loss: 0.657723\n",
            "Train Epoch: 9 [3840/6000 (64%)]\t Loss: 0.907808\n",
            "Train Epoch: 9 [4480/6000 (74%)]\t Loss: 0.947496\n",
            "Train Epoch: 9 [5120/6000 (85%)]\t Loss: 0.919327\n",
            "Train Epoch: 9 [5760/6000 (96%)]\t Loss: 1.030723\n",
            "Iteration   4, Average Loss 0.99\n",
            "Train Epoch: 0 [0/6000 (0%)]\t Loss: 0.879819\n",
            "Train Epoch: 0 [640/6000 (11%)]\t Loss: 0.799633\n",
            "Train Epoch: 0 [1280/6000 (21%)]\t Loss: 0.853735\n",
            "Train Epoch: 0 [1920/6000 (32%)]\t Loss: 0.923406\n",
            "Train Epoch: 0 [2560/6000 (43%)]\t Loss: 0.902340\n",
            "Train Epoch: 0 [3200/6000 (53%)]\t Loss: 0.837530\n",
            "Train Epoch: 0 [3840/6000 (64%)]\t Loss: 1.099505\n",
            "Train Epoch: 0 [4480/6000 (74%)]\t Loss: 1.091310\n",
            "Train Epoch: 0 [5120/6000 (85%)]\t Loss: 0.869149\n",
            "Train Epoch: 0 [5760/6000 (96%)]\t Loss: 1.077436\n",
            "Train Epoch: 1 [0/6000 (0%)]\t Loss: 1.191360\n",
            "Train Epoch: 1 [640/6000 (11%)]\t Loss: 1.084541\n",
            "Train Epoch: 1 [1280/6000 (21%)]\t Loss: 1.044514\n",
            "Train Epoch: 1 [1920/6000 (32%)]\t Loss: 0.950169\n",
            "Train Epoch: 1 [2560/6000 (43%)]\t Loss: 0.836174\n",
            "Train Epoch: 1 [3200/6000 (53%)]\t Loss: 0.941482\n",
            "Train Epoch: 1 [3840/6000 (64%)]\t Loss: 0.949286\n",
            "Train Epoch: 1 [4480/6000 (74%)]\t Loss: 0.829903\n",
            "Train Epoch: 1 [5120/6000 (85%)]\t Loss: 1.003905\n",
            "Train Epoch: 1 [5760/6000 (96%)]\t Loss: 0.933181\n",
            "Train Epoch: 2 [0/6000 (0%)]\t Loss: 1.079564\n",
            "Train Epoch: 2 [640/6000 (11%)]\t Loss: 1.014805\n",
            "Train Epoch: 2 [1280/6000 (21%)]\t Loss: 0.957197\n",
            "Train Epoch: 2 [1920/6000 (32%)]\t Loss: 1.037707\n",
            "Train Epoch: 2 [2560/6000 (43%)]\t Loss: 0.621058\n",
            "Train Epoch: 2 [3200/6000 (53%)]\t Loss: 1.135967\n",
            "Train Epoch: 2 [3840/6000 (64%)]\t Loss: 1.050531\n",
            "Train Epoch: 2 [4480/6000 (74%)]\t Loss: 0.999720\n",
            "Train Epoch: 2 [5120/6000 (85%)]\t Loss: 0.871769\n",
            "Train Epoch: 2 [5760/6000 (96%)]\t Loss: 0.857467\n",
            "Train Epoch: 3 [0/6000 (0%)]\t Loss: 0.991615\n",
            "Train Epoch: 3 [640/6000 (11%)]\t Loss: 1.043142\n",
            "Train Epoch: 3 [1280/6000 (21%)]\t Loss: 0.973881\n",
            "Train Epoch: 3 [1920/6000 (32%)]\t Loss: 0.830232\n",
            "Train Epoch: 3 [2560/6000 (43%)]\t Loss: 0.772526\n",
            "Train Epoch: 3 [3200/6000 (53%)]\t Loss: 1.097827\n",
            "Train Epoch: 3 [3840/6000 (64%)]\t Loss: 1.046508\n",
            "Train Epoch: 3 [4480/6000 (74%)]\t Loss: 1.052844\n",
            "Train Epoch: 3 [5120/6000 (85%)]\t Loss: 0.754281\n",
            "Train Epoch: 3 [5760/6000 (96%)]\t Loss: 0.921639\n",
            "Train Epoch: 4 [0/6000 (0%)]\t Loss: 0.914313\n",
            "Train Epoch: 4 [640/6000 (11%)]\t Loss: 0.836220\n",
            "Train Epoch: 4 [1280/6000 (21%)]\t Loss: 0.950921\n",
            "Train Epoch: 4 [1920/6000 (32%)]\t Loss: 0.792505\n",
            "Train Epoch: 4 [2560/6000 (43%)]\t Loss: 0.948093\n",
            "Train Epoch: 4 [3200/6000 (53%)]\t Loss: 1.386336\n",
            "Train Epoch: 4 [3840/6000 (64%)]\t Loss: 0.845980\n",
            "Train Epoch: 4 [4480/6000 (74%)]\t Loss: 0.898554\n",
            "Train Epoch: 4 [5120/6000 (85%)]\t Loss: 0.895680\n",
            "Train Epoch: 4 [5760/6000 (96%)]\t Loss: 1.235546\n",
            "Train Epoch: 5 [0/6000 (0%)]\t Loss: 0.924226\n",
            "Train Epoch: 5 [640/6000 (11%)]\t Loss: 0.956498\n",
            "Train Epoch: 5 [1280/6000 (21%)]\t Loss: 0.891290\n",
            "Train Epoch: 5 [1920/6000 (32%)]\t Loss: 0.835658\n",
            "Train Epoch: 5 [2560/6000 (43%)]\t Loss: 1.003371\n",
            "Train Epoch: 5 [3200/6000 (53%)]\t Loss: 0.875799\n",
            "Train Epoch: 5 [3840/6000 (64%)]\t Loss: 0.839892\n",
            "Train Epoch: 5 [4480/6000 (74%)]\t Loss: 0.983879\n",
            "Train Epoch: 5 [5120/6000 (85%)]\t Loss: 1.041585\n",
            "Train Epoch: 5 [5760/6000 (96%)]\t Loss: 0.926901\n",
            "Train Epoch: 6 [0/6000 (0%)]\t Loss: 1.059885\n",
            "Train Epoch: 6 [640/6000 (11%)]\t Loss: 0.924893\n",
            "Train Epoch: 6 [1280/6000 (21%)]\t Loss: 1.091848\n",
            "Train Epoch: 6 [1920/6000 (32%)]\t Loss: 0.821960\n",
            "Train Epoch: 6 [2560/6000 (43%)]\t Loss: 0.908913\n",
            "Train Epoch: 6 [3200/6000 (53%)]\t Loss: 1.217071\n",
            "Train Epoch: 6 [3840/6000 (64%)]\t Loss: 0.837648\n",
            "Train Epoch: 6 [4480/6000 (74%)]\t Loss: 0.769374\n",
            "Train Epoch: 6 [5120/6000 (85%)]\t Loss: 0.907587\n",
            "Train Epoch: 6 [5760/6000 (96%)]\t Loss: 0.892646\n",
            "Train Epoch: 7 [0/6000 (0%)]\t Loss: 0.955160\n",
            "Train Epoch: 7 [640/6000 (11%)]\t Loss: 0.991240\n",
            "Train Epoch: 7 [1280/6000 (21%)]\t Loss: 0.909580\n",
            "Train Epoch: 7 [1920/6000 (32%)]\t Loss: 0.695412\n",
            "Train Epoch: 7 [2560/6000 (43%)]\t Loss: 0.733584\n",
            "Train Epoch: 7 [3200/6000 (53%)]\t Loss: 1.062333\n",
            "Train Epoch: 7 [3840/6000 (64%)]\t Loss: 0.805799\n",
            "Train Epoch: 7 [4480/6000 (74%)]\t Loss: 0.959233\n",
            "Train Epoch: 7 [5120/6000 (85%)]\t Loss: 0.894653\n",
            "Train Epoch: 7 [5760/6000 (96%)]\t Loss: 0.886710\n",
            "Train Epoch: 8 [0/6000 (0%)]\t Loss: 1.016040\n",
            "Train Epoch: 8 [640/6000 (11%)]\t Loss: 1.016467\n",
            "Train Epoch: 8 [1280/6000 (21%)]\t Loss: 1.012039\n",
            "Train Epoch: 8 [1920/6000 (32%)]\t Loss: 0.895722\n",
            "Train Epoch: 8 [2560/6000 (43%)]\t Loss: 0.867192\n",
            "Train Epoch: 8 [3200/6000 (53%)]\t Loss: 0.768228\n",
            "Train Epoch: 8 [3840/6000 (64%)]\t Loss: 0.847684\n",
            "Train Epoch: 8 [4480/6000 (74%)]\t Loss: 1.052474\n",
            "Train Epoch: 8 [5120/6000 (85%)]\t Loss: 0.737763\n",
            "Train Epoch: 8 [5760/6000 (96%)]\t Loss: 0.877487\n",
            "Train Epoch: 9 [0/6000 (0%)]\t Loss: 1.056584\n",
            "Train Epoch: 9 [640/6000 (11%)]\t Loss: 0.761950\n",
            "Train Epoch: 9 [1280/6000 (21%)]\t Loss: 0.883539\n",
            "Train Epoch: 9 [1920/6000 (32%)]\t Loss: 1.169252\n",
            "Train Epoch: 9 [2560/6000 (43%)]\t Loss: 0.941109\n",
            "Train Epoch: 9 [3200/6000 (53%)]\t Loss: 0.826011\n",
            "Train Epoch: 9 [3840/6000 (64%)]\t Loss: 1.127156\n",
            "Train Epoch: 9 [4480/6000 (74%)]\t Loss: 1.466269\n",
            "Train Epoch: 9 [5120/6000 (85%)]\t Loss: 0.683416\n",
            "Train Epoch: 9 [5760/6000 (96%)]\t Loss: 1.048731\n",
            "Iteration   5, Average Loss 0.95\n",
            "Train Epoch: 0 [0/6000 (0%)]\t Loss: 1.049438\n",
            "Train Epoch: 0 [640/6000 (11%)]\t Loss: 1.126089\n",
            "Train Epoch: 0 [1280/6000 (21%)]\t Loss: 1.241074\n",
            "Train Epoch: 0 [1920/6000 (32%)]\t Loss: 0.941794\n",
            "Train Epoch: 0 [2560/6000 (43%)]\t Loss: 0.998705\n",
            "Train Epoch: 0 [3200/6000 (53%)]\t Loss: 1.057026\n",
            "Train Epoch: 0 [3840/6000 (64%)]\t Loss: 1.055058\n",
            "Train Epoch: 0 [4480/6000 (74%)]\t Loss: 0.732111\n",
            "Train Epoch: 0 [5120/6000 (85%)]\t Loss: 0.927834\n",
            "Train Epoch: 0 [5760/6000 (96%)]\t Loss: 0.726292\n",
            "Train Epoch: 1 [0/6000 (0%)]\t Loss: 1.209899\n",
            "Train Epoch: 1 [640/6000 (11%)]\t Loss: 0.760450\n",
            "Train Epoch: 1 [1280/6000 (21%)]\t Loss: 1.159176\n",
            "Train Epoch: 1 [1920/6000 (32%)]\t Loss: 0.620749\n",
            "Train Epoch: 1 [2560/6000 (43%)]\t Loss: 0.798326\n",
            "Train Epoch: 1 [3200/6000 (53%)]\t Loss: 1.064326\n",
            "Train Epoch: 1 [3840/6000 (64%)]\t Loss: 0.875141\n",
            "Train Epoch: 1 [4480/6000 (74%)]\t Loss: 0.975125\n",
            "Train Epoch: 1 [5120/6000 (85%)]\t Loss: 1.062302\n",
            "Train Epoch: 1 [5760/6000 (96%)]\t Loss: 1.266992\n",
            "Train Epoch: 2 [0/6000 (0%)]\t Loss: 1.183486\n",
            "Train Epoch: 2 [640/6000 (11%)]\t Loss: 0.881510\n",
            "Train Epoch: 2 [1280/6000 (21%)]\t Loss: 1.130014\n",
            "Train Epoch: 2 [1920/6000 (32%)]\t Loss: 0.649309\n",
            "Train Epoch: 2 [2560/6000 (43%)]\t Loss: 1.122869\n",
            "Train Epoch: 2 [3200/6000 (53%)]\t Loss: 0.840900\n",
            "Train Epoch: 2 [3840/6000 (64%)]\t Loss: 0.939985\n",
            "Train Epoch: 2 [4480/6000 (74%)]\t Loss: 0.689056\n",
            "Train Epoch: 2 [5120/6000 (85%)]\t Loss: 0.732433\n",
            "Train Epoch: 2 [5760/6000 (96%)]\t Loss: 0.649419\n",
            "Train Epoch: 3 [0/6000 (0%)]\t Loss: 0.767726\n",
            "Train Epoch: 3 [640/6000 (11%)]\t Loss: 0.980848\n",
            "Train Epoch: 3 [1280/6000 (21%)]\t Loss: 0.878344\n",
            "Train Epoch: 3 [1920/6000 (32%)]\t Loss: 0.885696\n",
            "Train Epoch: 3 [2560/6000 (43%)]\t Loss: 0.824816\n",
            "Train Epoch: 3 [3200/6000 (53%)]\t Loss: 0.852514\n",
            "Train Epoch: 3 [3840/6000 (64%)]\t Loss: 0.945579\n",
            "Train Epoch: 3 [4480/6000 (74%)]\t Loss: 0.952736\n",
            "Train Epoch: 3 [5120/6000 (85%)]\t Loss: 0.856514\n",
            "Train Epoch: 3 [5760/6000 (96%)]\t Loss: 1.414301\n",
            "Train Epoch: 4 [0/6000 (0%)]\t Loss: 0.985068\n",
            "Train Epoch: 4 [640/6000 (11%)]\t Loss: 0.919467\n",
            "Train Epoch: 4 [1280/6000 (21%)]\t Loss: 0.907433\n",
            "Train Epoch: 4 [1920/6000 (32%)]\t Loss: 0.951560\n",
            "Train Epoch: 4 [2560/6000 (43%)]\t Loss: 0.772849\n",
            "Train Epoch: 4 [3200/6000 (53%)]\t Loss: 0.736269\n",
            "Train Epoch: 4 [3840/6000 (64%)]\t Loss: 0.936409\n",
            "Train Epoch: 4 [4480/6000 (74%)]\t Loss: 0.758144\n",
            "Train Epoch: 4 [5120/6000 (85%)]\t Loss: 0.878762\n",
            "Train Epoch: 4 [5760/6000 (96%)]\t Loss: 0.966886\n",
            "Train Epoch: 5 [0/6000 (0%)]\t Loss: 0.905250\n",
            "Train Epoch: 5 [640/6000 (11%)]\t Loss: 0.992037\n",
            "Train Epoch: 5 [1280/6000 (21%)]\t Loss: 0.806760\n",
            "Train Epoch: 5 [1920/6000 (32%)]\t Loss: 0.802255\n",
            "Train Epoch: 5 [2560/6000 (43%)]\t Loss: 1.017923\n",
            "Train Epoch: 5 [3200/6000 (53%)]\t Loss: 0.881938\n",
            "Train Epoch: 5 [3840/6000 (64%)]\t Loss: 1.064900\n",
            "Train Epoch: 5 [4480/6000 (74%)]\t Loss: 1.048666\n",
            "Train Epoch: 5 [5120/6000 (85%)]\t Loss: 0.935800\n",
            "Train Epoch: 5 [5760/6000 (96%)]\t Loss: 0.761778\n",
            "Train Epoch: 6 [0/6000 (0%)]\t Loss: 0.884947\n",
            "Train Epoch: 6 [640/6000 (11%)]\t Loss: 1.068716\n",
            "Train Epoch: 6 [1280/6000 (21%)]\t Loss: 0.874920\n",
            "Train Epoch: 6 [1920/6000 (32%)]\t Loss: 0.762239\n",
            "Train Epoch: 6 [2560/6000 (43%)]\t Loss: 0.840241\n",
            "Train Epoch: 6 [3200/6000 (53%)]\t Loss: 0.957837\n",
            "Train Epoch: 6 [3840/6000 (64%)]\t Loss: 1.116971\n",
            "Train Epoch: 6 [4480/6000 (74%)]\t Loss: 0.850497\n",
            "Train Epoch: 6 [5120/6000 (85%)]\t Loss: 0.904205\n",
            "Train Epoch: 6 [5760/6000 (96%)]\t Loss: 1.014700\n",
            "Train Epoch: 7 [0/6000 (0%)]\t Loss: 0.942376\n",
            "Train Epoch: 7 [640/6000 (11%)]\t Loss: 0.870666\n",
            "Train Epoch: 7 [1280/6000 (21%)]\t Loss: 0.940051\n",
            "Train Epoch: 7 [1920/6000 (32%)]\t Loss: 0.758692\n",
            "Train Epoch: 7 [2560/6000 (43%)]\t Loss: 1.008253\n",
            "Train Epoch: 7 [3200/6000 (53%)]\t Loss: 1.003490\n",
            "Train Epoch: 7 [3840/6000 (64%)]\t Loss: 0.802045\n",
            "Train Epoch: 7 [4480/6000 (74%)]\t Loss: 1.012642\n",
            "Train Epoch: 7 [5120/6000 (85%)]\t Loss: 1.027052\n",
            "Train Epoch: 7 [5760/6000 (96%)]\t Loss: 0.550690\n",
            "Train Epoch: 8 [0/6000 (0%)]\t Loss: 0.731573\n",
            "Train Epoch: 8 [640/6000 (11%)]\t Loss: 0.955856\n",
            "Train Epoch: 8 [1280/6000 (21%)]\t Loss: 0.879799\n",
            "Train Epoch: 8 [1920/6000 (32%)]\t Loss: 1.055750\n",
            "Train Epoch: 8 [2560/6000 (43%)]\t Loss: 0.685458\n",
            "Train Epoch: 8 [3200/6000 (53%)]\t Loss: 0.766463\n",
            "Train Epoch: 8 [3840/6000 (64%)]\t Loss: 0.615984\n",
            "Train Epoch: 8 [4480/6000 (74%)]\t Loss: 1.093181\n",
            "Train Epoch: 8 [5120/6000 (85%)]\t Loss: 0.976343\n",
            "Train Epoch: 8 [5760/6000 (96%)]\t Loss: 0.935599\n",
            "Train Epoch: 9 [0/6000 (0%)]\t Loss: 0.761406\n",
            "Train Epoch: 9 [640/6000 (11%)]\t Loss: 0.867362\n",
            "Train Epoch: 9 [1280/6000 (21%)]\t Loss: 0.651527\n",
            "Train Epoch: 9 [1920/6000 (32%)]\t Loss: 1.078724\n",
            "Train Epoch: 9 [2560/6000 (43%)]\t Loss: 0.879470\n",
            "Train Epoch: 9 [3200/6000 (53%)]\t Loss: 1.011823\n",
            "Train Epoch: 9 [3840/6000 (64%)]\t Loss: 0.841390\n",
            "Train Epoch: 9 [4480/6000 (74%)]\t Loss: 0.721501\n",
            "Train Epoch: 9 [5120/6000 (85%)]\t Loss: 0.805186\n",
            "Train Epoch: 9 [5760/6000 (96%)]\t Loss: 0.993133\n",
            "Iteration   6, Average Loss 0.92\n",
            "Train Epoch: 0 [0/6000 (0%)]\t Loss: 0.936280\n",
            "Train Epoch: 0 [640/6000 (11%)]\t Loss: 0.908463\n",
            "Train Epoch: 0 [1280/6000 (21%)]\t Loss: 1.122404\n",
            "Train Epoch: 0 [1920/6000 (32%)]\t Loss: 0.928580\n",
            "Train Epoch: 0 [2560/6000 (43%)]\t Loss: 0.824967\n",
            "Train Epoch: 0 [3200/6000 (53%)]\t Loss: 0.950231\n",
            "Train Epoch: 0 [3840/6000 (64%)]\t Loss: 0.912149\n",
            "Train Epoch: 0 [4480/6000 (74%)]\t Loss: 0.681135\n",
            "Train Epoch: 0 [5120/6000 (85%)]\t Loss: 0.941109\n",
            "Train Epoch: 0 [5760/6000 (96%)]\t Loss: 1.078668\n",
            "Train Epoch: 1 [0/6000 (0%)]\t Loss: 1.052957\n",
            "Train Epoch: 1 [640/6000 (11%)]\t Loss: 0.823287\n",
            "Train Epoch: 1 [1280/6000 (21%)]\t Loss: 0.796112\n",
            "Train Epoch: 1 [1920/6000 (32%)]\t Loss: 0.931516\n",
            "Train Epoch: 1 [2560/6000 (43%)]\t Loss: 0.766020\n",
            "Train Epoch: 1 [3200/6000 (53%)]\t Loss: 1.000170\n",
            "Train Epoch: 1 [3840/6000 (64%)]\t Loss: 0.906973\n",
            "Train Epoch: 1 [4480/6000 (74%)]\t Loss: 1.052152\n",
            "Train Epoch: 1 [5120/6000 (85%)]\t Loss: 1.325509\n",
            "Train Epoch: 1 [5760/6000 (96%)]\t Loss: 0.911916\n",
            "Train Epoch: 2 [0/6000 (0%)]\t Loss: 0.806285\n",
            "Train Epoch: 2 [640/6000 (11%)]\t Loss: 1.011572\n",
            "Train Epoch: 2 [1280/6000 (21%)]\t Loss: 0.955878\n",
            "Train Epoch: 2 [1920/6000 (32%)]\t Loss: 0.690578\n",
            "Train Epoch: 2 [2560/6000 (43%)]\t Loss: 0.726406\n",
            "Train Epoch: 2 [3200/6000 (53%)]\t Loss: 0.913667\n",
            "Train Epoch: 2 [3840/6000 (64%)]\t Loss: 0.896424\n",
            "Train Epoch: 2 [4480/6000 (74%)]\t Loss: 0.975924\n",
            "Train Epoch: 2 [5120/6000 (85%)]\t Loss: 0.871667\n",
            "Train Epoch: 2 [5760/6000 (96%)]\t Loss: 0.841141\n",
            "Train Epoch: 3 [0/6000 (0%)]\t Loss: 1.146853\n",
            "Train Epoch: 3 [640/6000 (11%)]\t Loss: 1.181791\n",
            "Train Epoch: 3 [1280/6000 (21%)]\t Loss: 1.050330\n",
            "Train Epoch: 3 [1920/6000 (32%)]\t Loss: 0.752374\n",
            "Train Epoch: 3 [2560/6000 (43%)]\t Loss: 0.885250\n",
            "Train Epoch: 3 [3200/6000 (53%)]\t Loss: 1.117722\n",
            "Train Epoch: 3 [3840/6000 (64%)]\t Loss: 0.895078\n",
            "Train Epoch: 3 [4480/6000 (74%)]\t Loss: 0.805729\n",
            "Train Epoch: 3 [5120/6000 (85%)]\t Loss: 1.008791\n",
            "Train Epoch: 3 [5760/6000 (96%)]\t Loss: 0.803216\n",
            "Train Epoch: 4 [0/6000 (0%)]\t Loss: 1.036824\n",
            "Train Epoch: 4 [640/6000 (11%)]\t Loss: 0.894820\n",
            "Train Epoch: 4 [1280/6000 (21%)]\t Loss: 0.854834\n",
            "Train Epoch: 4 [1920/6000 (32%)]\t Loss: 0.948616\n",
            "Train Epoch: 4 [2560/6000 (43%)]\t Loss: 1.054180\n",
            "Train Epoch: 4 [3200/6000 (53%)]\t Loss: 0.945731\n",
            "Train Epoch: 4 [3840/6000 (64%)]\t Loss: 1.179416\n",
            "Train Epoch: 4 [4480/6000 (74%)]\t Loss: 1.135432\n",
            "Train Epoch: 4 [5120/6000 (85%)]\t Loss: 0.999565\n",
            "Train Epoch: 4 [5760/6000 (96%)]\t Loss: 1.036635\n",
            "Train Epoch: 5 [0/6000 (0%)]\t Loss: 0.558445\n",
            "Train Epoch: 5 [640/6000 (11%)]\t Loss: 0.706186\n",
            "Train Epoch: 5 [1280/6000 (21%)]\t Loss: 1.056917\n",
            "Train Epoch: 5 [1920/6000 (32%)]\t Loss: 0.868638\n",
            "Train Epoch: 5 [2560/6000 (43%)]\t Loss: 0.989235\n",
            "Train Epoch: 5 [3200/6000 (53%)]\t Loss: 1.016746\n",
            "Train Epoch: 5 [3840/6000 (64%)]\t Loss: 0.722929\n",
            "Train Epoch: 5 [4480/6000 (74%)]\t Loss: 0.983631\n",
            "Train Epoch: 5 [5120/6000 (85%)]\t Loss: 1.040917\n",
            "Train Epoch: 5 [5760/6000 (96%)]\t Loss: 0.819148\n",
            "Train Epoch: 6 [0/6000 (0%)]\t Loss: 0.804615\n",
            "Train Epoch: 6 [640/6000 (11%)]\t Loss: 0.758298\n",
            "Train Epoch: 6 [1280/6000 (21%)]\t Loss: 0.982764\n",
            "Train Epoch: 6 [1920/6000 (32%)]\t Loss: 0.873650\n",
            "Train Epoch: 6 [2560/6000 (43%)]\t Loss: 1.124221\n",
            "Train Epoch: 6 [3200/6000 (53%)]\t Loss: 0.694591\n",
            "Train Epoch: 6 [3840/6000 (64%)]\t Loss: 1.020163\n",
            "Train Epoch: 6 [4480/6000 (74%)]\t Loss: 1.062437\n",
            "Train Epoch: 6 [5120/6000 (85%)]\t Loss: 1.047511\n",
            "Train Epoch: 6 [5760/6000 (96%)]\t Loss: 0.869061\n",
            "Train Epoch: 7 [0/6000 (0%)]\t Loss: 0.856947\n",
            "Train Epoch: 7 [640/6000 (11%)]\t Loss: 1.203368\n",
            "Train Epoch: 7 [1280/6000 (21%)]\t Loss: 0.731533\n",
            "Train Epoch: 7 [1920/6000 (32%)]\t Loss: 0.974942\n",
            "Train Epoch: 7 [2560/6000 (43%)]\t Loss: 1.168782\n",
            "Train Epoch: 7 [3200/6000 (53%)]\t Loss: 0.982048\n",
            "Train Epoch: 7 [3840/6000 (64%)]\t Loss: 1.013535\n",
            "Train Epoch: 7 [4480/6000 (74%)]\t Loss: 0.914108\n",
            "Train Epoch: 7 [5120/6000 (85%)]\t Loss: 1.113591\n",
            "Train Epoch: 7 [5760/6000 (96%)]\t Loss: 0.705233\n",
            "Train Epoch: 8 [0/6000 (0%)]\t Loss: 1.059226\n",
            "Train Epoch: 8 [640/6000 (11%)]\t Loss: 1.013828\n",
            "Train Epoch: 8 [1280/6000 (21%)]\t Loss: 1.128448\n",
            "Train Epoch: 8 [1920/6000 (32%)]\t Loss: 1.094476\n",
            "Train Epoch: 8 [2560/6000 (43%)]\t Loss: 0.941578\n",
            "Train Epoch: 8 [3200/6000 (53%)]\t Loss: 1.026911\n",
            "Train Epoch: 8 [3840/6000 (64%)]\t Loss: 0.727872\n",
            "Train Epoch: 8 [4480/6000 (74%)]\t Loss: 0.916422\n",
            "Train Epoch: 8 [5120/6000 (85%)]\t Loss: 0.728117\n",
            "Train Epoch: 8 [5760/6000 (96%)]\t Loss: 0.959378\n",
            "Train Epoch: 9 [0/6000 (0%)]\t Loss: 0.880963\n",
            "Train Epoch: 9 [640/6000 (11%)]\t Loss: 0.984288\n",
            "Train Epoch: 9 [1280/6000 (21%)]\t Loss: 0.901908\n",
            "Train Epoch: 9 [1920/6000 (32%)]\t Loss: 0.867539\n",
            "Train Epoch: 9 [2560/6000 (43%)]\t Loss: 0.828266\n",
            "Train Epoch: 9 [3200/6000 (53%)]\t Loss: 0.936600\n",
            "Train Epoch: 9 [3840/6000 (64%)]\t Loss: 1.080943\n",
            "Train Epoch: 9 [4480/6000 (74%)]\t Loss: 0.989631\n",
            "Train Epoch: 9 [5120/6000 (85%)]\t Loss: 0.742005\n",
            "Train Epoch: 9 [5760/6000 (96%)]\t Loss: 1.053140\n",
            "Iteration   7, Average Loss 0.95\n",
            "Train Epoch: 0 [0/6000 (0%)]\t Loss: 1.183689\n",
            "Train Epoch: 0 [640/6000 (11%)]\t Loss: 0.974014\n",
            "Train Epoch: 0 [1280/6000 (21%)]\t Loss: 0.849738\n",
            "Train Epoch: 0 [1920/6000 (32%)]\t Loss: 1.084028\n",
            "Train Epoch: 0 [2560/6000 (43%)]\t Loss: 0.855965\n",
            "Train Epoch: 0 [3200/6000 (53%)]\t Loss: 1.055292\n",
            "Train Epoch: 0 [3840/6000 (64%)]\t Loss: 0.805602\n",
            "Train Epoch: 0 [4480/6000 (74%)]\t Loss: 1.039173\n",
            "Train Epoch: 0 [5120/6000 (85%)]\t Loss: 1.288638\n",
            "Train Epoch: 0 [5760/6000 (96%)]\t Loss: 1.067300\n",
            "Train Epoch: 1 [0/6000 (0%)]\t Loss: 0.943281\n",
            "Train Epoch: 1 [640/6000 (11%)]\t Loss: 1.123612\n",
            "Train Epoch: 1 [1280/6000 (21%)]\t Loss: 0.802077\n",
            "Train Epoch: 1 [1920/6000 (32%)]\t Loss: 0.974056\n",
            "Train Epoch: 1 [2560/6000 (43%)]\t Loss: 0.984451\n",
            "Train Epoch: 1 [3200/6000 (53%)]\t Loss: 0.923494\n",
            "Train Epoch: 1 [3840/6000 (64%)]\t Loss: 0.665315\n",
            "Train Epoch: 1 [4480/6000 (74%)]\t Loss: 0.845537\n",
            "Train Epoch: 1 [5120/6000 (85%)]\t Loss: 1.264220\n",
            "Train Epoch: 1 [5760/6000 (96%)]\t Loss: 1.285090\n",
            "Train Epoch: 2 [0/6000 (0%)]\t Loss: 1.121818\n",
            "Train Epoch: 2 [640/6000 (11%)]\t Loss: 0.861852\n",
            "Train Epoch: 2 [1280/6000 (21%)]\t Loss: 1.305886\n",
            "Train Epoch: 2 [1920/6000 (32%)]\t Loss: 1.047598\n",
            "Train Epoch: 2 [2560/6000 (43%)]\t Loss: 1.036723\n",
            "Train Epoch: 2 [3200/6000 (53%)]\t Loss: 1.019071\n",
            "Train Epoch: 2 [3840/6000 (64%)]\t Loss: 1.169122\n",
            "Train Epoch: 2 [4480/6000 (74%)]\t Loss: 1.248158\n",
            "Train Epoch: 2 [5120/6000 (85%)]\t Loss: 1.154817\n",
            "Train Epoch: 2 [5760/6000 (96%)]\t Loss: 1.245211\n",
            "Train Epoch: 3 [0/6000 (0%)]\t Loss: 1.014109\n",
            "Train Epoch: 3 [640/6000 (11%)]\t Loss: 1.153856\n",
            "Train Epoch: 3 [1280/6000 (21%)]\t Loss: 1.084341\n",
            "Train Epoch: 3 [1920/6000 (32%)]\t Loss: 0.908002\n",
            "Train Epoch: 3 [2560/6000 (43%)]\t Loss: 0.937265\n",
            "Train Epoch: 3 [3200/6000 (53%)]\t Loss: 0.701713\n",
            "Train Epoch: 3 [3840/6000 (64%)]\t Loss: 0.784266\n",
            "Train Epoch: 3 [4480/6000 (74%)]\t Loss: 1.079686\n",
            "Train Epoch: 3 [5120/6000 (85%)]\t Loss: 1.206465\n",
            "Train Epoch: 3 [5760/6000 (96%)]\t Loss: 0.981359\n",
            "Train Epoch: 4 [0/6000 (0%)]\t Loss: 0.723485\n",
            "Train Epoch: 4 [640/6000 (11%)]\t Loss: 0.843845\n",
            "Train Epoch: 4 [1280/6000 (21%)]\t Loss: 0.900884\n",
            "Train Epoch: 4 [1920/6000 (32%)]\t Loss: 1.088549\n",
            "Train Epoch: 4 [2560/6000 (43%)]\t Loss: 0.955135\n",
            "Train Epoch: 4 [3200/6000 (53%)]\t Loss: 0.913803\n",
            "Train Epoch: 4 [3840/6000 (64%)]\t Loss: 0.983012\n",
            "Train Epoch: 4 [4480/6000 (74%)]\t Loss: 0.939179\n",
            "Train Epoch: 4 [5120/6000 (85%)]\t Loss: 0.697175\n",
            "Train Epoch: 4 [5760/6000 (96%)]\t Loss: 0.908901\n",
            "Train Epoch: 5 [0/6000 (0%)]\t Loss: 0.829681\n",
            "Train Epoch: 5 [640/6000 (11%)]\t Loss: 0.964430\n",
            "Train Epoch: 5 [1280/6000 (21%)]\t Loss: 0.764415\n",
            "Train Epoch: 5 [1920/6000 (32%)]\t Loss: 1.154602\n",
            "Train Epoch: 5 [2560/6000 (43%)]\t Loss: 1.011146\n",
            "Train Epoch: 5 [3200/6000 (53%)]\t Loss: 0.877800\n",
            "Train Epoch: 5 [3840/6000 (64%)]\t Loss: 0.906627\n",
            "Train Epoch: 5 [4480/6000 (74%)]\t Loss: 1.123611\n",
            "Train Epoch: 5 [5120/6000 (85%)]\t Loss: 0.694052\n",
            "Train Epoch: 5 [5760/6000 (96%)]\t Loss: 0.962577\n",
            "Train Epoch: 6 [0/6000 (0%)]\t Loss: 0.980613\n",
            "Train Epoch: 6 [640/6000 (11%)]\t Loss: 1.116632\n",
            "Train Epoch: 6 [1280/6000 (21%)]\t Loss: 1.195449\n",
            "Train Epoch: 6 [1920/6000 (32%)]\t Loss: 0.972274\n",
            "Train Epoch: 6 [2560/6000 (43%)]\t Loss: 1.089043\n",
            "Train Epoch: 6 [3200/6000 (53%)]\t Loss: 0.872809\n",
            "Train Epoch: 6 [3840/6000 (64%)]\t Loss: 1.079984\n",
            "Train Epoch: 6 [4480/6000 (74%)]\t Loss: 1.062346\n",
            "Train Epoch: 6 [5120/6000 (85%)]\t Loss: 0.937963\n",
            "Train Epoch: 6 [5760/6000 (96%)]\t Loss: 1.096546\n",
            "Train Epoch: 7 [0/6000 (0%)]\t Loss: 0.940020\n",
            "Train Epoch: 7 [640/6000 (11%)]\t Loss: 1.054505\n",
            "Train Epoch: 7 [1280/6000 (21%)]\t Loss: 1.301262\n",
            "Train Epoch: 7 [1920/6000 (32%)]\t Loss: 1.085796\n",
            "Train Epoch: 7 [2560/6000 (43%)]\t Loss: 0.861239\n",
            "Train Epoch: 7 [3200/6000 (53%)]\t Loss: 0.837875\n",
            "Train Epoch: 7 [3840/6000 (64%)]\t Loss: 0.764201\n",
            "Train Epoch: 7 [4480/6000 (74%)]\t Loss: 1.047218\n",
            "Train Epoch: 7 [5120/6000 (85%)]\t Loss: 0.984602\n",
            "Train Epoch: 7 [5760/6000 (96%)]\t Loss: 0.944300\n",
            "Train Epoch: 8 [0/6000 (0%)]\t Loss: 0.985979\n",
            "Train Epoch: 8 [640/6000 (11%)]\t Loss: 0.945514\n",
            "Train Epoch: 8 [1280/6000 (21%)]\t Loss: 1.050031\n",
            "Train Epoch: 8 [1920/6000 (32%)]\t Loss: 0.945744\n",
            "Train Epoch: 8 [2560/6000 (43%)]\t Loss: 0.653389\n",
            "Train Epoch: 8 [3200/6000 (53%)]\t Loss: 0.764790\n",
            "Train Epoch: 8 [3840/6000 (64%)]\t Loss: 0.911148\n",
            "Train Epoch: 8 [4480/6000 (74%)]\t Loss: 0.978976\n",
            "Train Epoch: 8 [5120/6000 (85%)]\t Loss: 1.019214\n",
            "Train Epoch: 8 [5760/6000 (96%)]\t Loss: 1.018618\n",
            "Train Epoch: 9 [0/6000 (0%)]\t Loss: 0.840159\n",
            "Train Epoch: 9 [640/6000 (11%)]\t Loss: 0.830606\n",
            "Train Epoch: 9 [1280/6000 (21%)]\t Loss: 0.900351\n",
            "Train Epoch: 9 [1920/6000 (32%)]\t Loss: 0.935373\n",
            "Train Epoch: 9 [2560/6000 (43%)]\t Loss: 0.902295\n",
            "Train Epoch: 9 [3200/6000 (53%)]\t Loss: 1.124648\n",
            "Train Epoch: 9 [3840/6000 (64%)]\t Loss: 0.776755\n",
            "Train Epoch: 9 [4480/6000 (74%)]\t Loss: 1.051443\n",
            "Train Epoch: 9 [5120/6000 (85%)]\t Loss: 0.972515\n",
            "Train Epoch: 9 [5760/6000 (96%)]\t Loss: 0.800936\n",
            "Iteration   8, Average Loss 0.98\n",
            "Train Epoch: 0 [0/6000 (0%)]\t Loss: 0.899034\n",
            "Train Epoch: 0 [640/6000 (11%)]\t Loss: 0.660199\n",
            "Train Epoch: 0 [1280/6000 (21%)]\t Loss: 0.988818\n",
            "Train Epoch: 0 [1920/6000 (32%)]\t Loss: 0.918436\n",
            "Train Epoch: 0 [2560/6000 (43%)]\t Loss: 1.016316\n",
            "Train Epoch: 0 [3200/6000 (53%)]\t Loss: 0.851884\n",
            "Train Epoch: 0 [3840/6000 (64%)]\t Loss: 0.970517\n",
            "Train Epoch: 0 [4480/6000 (74%)]\t Loss: 1.094699\n",
            "Train Epoch: 0 [5120/6000 (85%)]\t Loss: 0.833436\n",
            "Train Epoch: 0 [5760/6000 (96%)]\t Loss: 0.914610\n",
            "Train Epoch: 1 [0/6000 (0%)]\t Loss: 0.795181\n",
            "Train Epoch: 1 [640/6000 (11%)]\t Loss: 0.872728\n",
            "Train Epoch: 1 [1280/6000 (21%)]\t Loss: 0.991035\n",
            "Train Epoch: 1 [1920/6000 (32%)]\t Loss: 0.930644\n",
            "Train Epoch: 1 [2560/6000 (43%)]\t Loss: 0.866758\n",
            "Train Epoch: 1 [3200/6000 (53%)]\t Loss: 1.008178\n",
            "Train Epoch: 1 [3840/6000 (64%)]\t Loss: 1.054413\n",
            "Train Epoch: 1 [4480/6000 (74%)]\t Loss: 0.762807\n",
            "Train Epoch: 1 [5120/6000 (85%)]\t Loss: 0.798706\n",
            "Train Epoch: 1 [5760/6000 (96%)]\t Loss: 0.966967\n",
            "Train Epoch: 2 [0/6000 (0%)]\t Loss: 1.149192\n",
            "Train Epoch: 2 [640/6000 (11%)]\t Loss: 0.779384\n",
            "Train Epoch: 2 [1280/6000 (21%)]\t Loss: 1.040265\n",
            "Train Epoch: 2 [1920/6000 (32%)]\t Loss: 0.901627\n",
            "Train Epoch: 2 [2560/6000 (43%)]\t Loss: 0.838171\n",
            "Train Epoch: 2 [3200/6000 (53%)]\t Loss: 0.765044\n",
            "Train Epoch: 2 [3840/6000 (64%)]\t Loss: 0.762091\n",
            "Train Epoch: 2 [4480/6000 (74%)]\t Loss: 1.063231\n",
            "Train Epoch: 2 [5120/6000 (85%)]\t Loss: 0.841004\n",
            "Train Epoch: 2 [5760/6000 (96%)]\t Loss: 0.984298\n",
            "Train Epoch: 3 [0/6000 (0%)]\t Loss: 0.832420\n",
            "Train Epoch: 3 [640/6000 (11%)]\t Loss: 0.937599\n",
            "Train Epoch: 3 [1280/6000 (21%)]\t Loss: 0.792107\n",
            "Train Epoch: 3 [1920/6000 (32%)]\t Loss: 0.722975\n",
            "Train Epoch: 3 [2560/6000 (43%)]\t Loss: 0.867738\n",
            "Train Epoch: 3 [3200/6000 (53%)]\t Loss: 1.101148\n",
            "Train Epoch: 3 [3840/6000 (64%)]\t Loss: 1.024918\n",
            "Train Epoch: 3 [4480/6000 (74%)]\t Loss: 1.016657\n",
            "Train Epoch: 3 [5120/6000 (85%)]\t Loss: 0.945004\n",
            "Train Epoch: 3 [5760/6000 (96%)]\t Loss: 1.049425\n",
            "Train Epoch: 4 [0/6000 (0%)]\t Loss: 0.905781\n",
            "Train Epoch: 4 [640/6000 (11%)]\t Loss: 1.134009\n",
            "Train Epoch: 4 [1280/6000 (21%)]\t Loss: 1.047999\n",
            "Train Epoch: 4 [1920/6000 (32%)]\t Loss: 0.974815\n",
            "Train Epoch: 4 [2560/6000 (43%)]\t Loss: 0.887809\n",
            "Train Epoch: 4 [3200/6000 (53%)]\t Loss: 0.993546\n",
            "Train Epoch: 4 [3840/6000 (64%)]\t Loss: 0.925296\n",
            "Train Epoch: 4 [4480/6000 (74%)]\t Loss: 0.903779\n",
            "Train Epoch: 4 [5120/6000 (85%)]\t Loss: 0.732527\n",
            "Train Epoch: 4 [5760/6000 (96%)]\t Loss: 1.018728\n",
            "Train Epoch: 5 [0/6000 (0%)]\t Loss: 0.842124\n",
            "Train Epoch: 5 [640/6000 (11%)]\t Loss: 0.979924\n",
            "Train Epoch: 5 [1280/6000 (21%)]\t Loss: 1.093568\n",
            "Train Epoch: 5 [1920/6000 (32%)]\t Loss: 0.957932\n",
            "Train Epoch: 5 [2560/6000 (43%)]\t Loss: 1.117328\n",
            "Train Epoch: 5 [3200/6000 (53%)]\t Loss: 0.722590\n",
            "Train Epoch: 5 [3840/6000 (64%)]\t Loss: 0.750319\n",
            "Train Epoch: 5 [4480/6000 (74%)]\t Loss: 1.046177\n",
            "Train Epoch: 5 [5120/6000 (85%)]\t Loss: 0.916086\n",
            "Train Epoch: 5 [5760/6000 (96%)]\t Loss: 0.873742\n",
            "Train Epoch: 6 [0/6000 (0%)]\t Loss: 1.119852\n",
            "Train Epoch: 6 [640/6000 (11%)]\t Loss: 0.654741\n",
            "Train Epoch: 6 [1280/6000 (21%)]\t Loss: 0.840403\n",
            "Train Epoch: 6 [1920/6000 (32%)]\t Loss: 0.935981\n",
            "Train Epoch: 6 [2560/6000 (43%)]\t Loss: 1.019091\n",
            "Train Epoch: 6 [3200/6000 (53%)]\t Loss: 0.986356\n",
            "Train Epoch: 6 [3840/6000 (64%)]\t Loss: 0.838050\n",
            "Train Epoch: 6 [4480/6000 (74%)]\t Loss: 0.771198\n",
            "Train Epoch: 6 [5120/6000 (85%)]\t Loss: 0.801950\n",
            "Train Epoch: 6 [5760/6000 (96%)]\t Loss: 0.908365\n",
            "Train Epoch: 7 [0/6000 (0%)]\t Loss: 0.799753\n",
            "Train Epoch: 7 [640/6000 (11%)]\t Loss: 1.118226\n",
            "Train Epoch: 7 [1280/6000 (21%)]\t Loss: 1.025302\n",
            "Train Epoch: 7 [1920/6000 (32%)]\t Loss: 0.799317\n",
            "Train Epoch: 7 [2560/6000 (43%)]\t Loss: 0.894169\n",
            "Train Epoch: 7 [3200/6000 (53%)]\t Loss: 0.802827\n",
            "Train Epoch: 7 [3840/6000 (64%)]\t Loss: 0.907579\n",
            "Train Epoch: 7 [4480/6000 (74%)]\t Loss: 1.033315\n",
            "Train Epoch: 7 [5120/6000 (85%)]\t Loss: 0.935865\n",
            "Train Epoch: 7 [5760/6000 (96%)]\t Loss: 0.878267\n",
            "Train Epoch: 8 [0/6000 (0%)]\t Loss: 1.060946\n",
            "Train Epoch: 8 [640/6000 (11%)]\t Loss: 0.940643\n",
            "Train Epoch: 8 [1280/6000 (21%)]\t Loss: 0.805023\n",
            "Train Epoch: 8 [1920/6000 (32%)]\t Loss: 1.052977\n",
            "Train Epoch: 8 [2560/6000 (43%)]\t Loss: 0.907571\n",
            "Train Epoch: 8 [3200/6000 (53%)]\t Loss: 0.902827\n",
            "Train Epoch: 8 [3840/6000 (64%)]\t Loss: 0.831078\n",
            "Train Epoch: 8 [4480/6000 (74%)]\t Loss: 0.552721\n",
            "Train Epoch: 8 [5120/6000 (85%)]\t Loss: 0.900094\n",
            "Train Epoch: 8 [5760/6000 (96%)]\t Loss: 0.759628\n",
            "Train Epoch: 9 [0/6000 (0%)]\t Loss: 0.878239\n",
            "Train Epoch: 9 [640/6000 (11%)]\t Loss: 0.974277\n",
            "Train Epoch: 9 [1280/6000 (21%)]\t Loss: 0.973244\n",
            "Train Epoch: 9 [1920/6000 (32%)]\t Loss: 0.950926\n",
            "Train Epoch: 9 [2560/6000 (43%)]\t Loss: 0.731861\n",
            "Train Epoch: 9 [3200/6000 (53%)]\t Loss: 0.686170\n",
            "Train Epoch: 9 [3840/6000 (64%)]\t Loss: 0.869270\n",
            "Train Epoch: 9 [4480/6000 (74%)]\t Loss: 0.832464\n",
            "Train Epoch: 9 [5120/6000 (85%)]\t Loss: 0.949033\n",
            "Train Epoch: 9 [5760/6000 (96%)]\t Loss: 1.074804\n",
            "Iteration   9, Average Loss 0.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTuV20HfvfKp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "4613aa41-9cba-40ea-91b6-97b74fd04edf"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.grid()\n",
        "plt.xlabel('Global Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(range(len(loss_training)), loss_training)\n",
        "plt.savefig('first-fed-averaging.png')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVVnwbTT2FAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testing_script(global_net, test_loader):\n",
        "  global_net.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "  for indx, (data, target) in enumerate(test_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    output = global_net(data)\n",
        "    test_loss+=F.nll_loss(output, target, reduction='sum').item()\n",
        "    pred = output.data.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
        "\n",
        "  test_loss/=len(test_loader.dataset)\n",
        "  accuracy = 100.00*correct/len(test_loader.dataset)\n",
        "  print('Test Set:  Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(test_loss, correct, len(test_loader.dataset),\n",
        "        accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kan75xTD6V6f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "37cc0ae2-eded-492a-d200-d87995556996"
      },
      "source": [
        "testing_script(global_net, test_loader)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Set:  Loss: 0.9517, Accuracy: 5960/10000 (60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCnmdkz06bso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}